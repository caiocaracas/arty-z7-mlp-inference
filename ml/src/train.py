import argparse
import json
import pathlib
import re
from typing import Iterable, Sequence

import numpy as np
from sklearn.datasets import make_classification
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler


SOLVER_CONFIGS = {
    "adam": dict(solver="adam", learning_rate_init=1e-3, beta_1=0.9, beta_2=0.999),
    # Desempenho inferior do SGD+momentum nesta topologia -> mantido apenas como referência histórica.
    # "sgd": dict(
    #     solver="sgd",
    #     learning_rate_init=5e-3,
    #     momentum=0.9,
    #     nesterovs_momentum=True,
    # ),
}


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Train and export a tiny MLP.")
    parser.add_argument("--hidden-units", type=int, default=32, help="Number of hidden neurons.")
    parser.add_argument("--solver", choices=["auto", "adam"], default="auto",
                        help="Select optimizer or let the script pick the best.")
    parser.add_argument("--max-iter", type=int, default=200, help="Maximum training iterations.")
    parser.add_argument("--random-state", type=int, default=42, help="Random seed for reproducibility.")
    parser.add_argument("--n-samples", type=int, default=4000, help="Synthetic samples to generate.")
    parser.add_argument("--n-features", type=int, default=16, help="Total number of input features.")
    parser.add_argument("--n-informative", type=int, default=12,
                        help="Informative features in the synthetic dataset.")
    parser.add_argument("--n-classes", type=int, default=5, help="Number of target classes.")
    parser.add_argument("--test-size", type=float, default=0.2, help="Proportion reserved for validation.")
    parser.add_argument("--export-prefix", type=str, default="mlp_model",
                        help="Base name for the exported artifacts.")
    parser.add_argument("--output-dir", type=pathlib.Path, default=pathlib.Path("export"),
                        help="Directory where artifacts will be written.")
    parser.add_argument("--input", type=str,
                        help="Optional comma-separated list or JSON file with a vector to run through the MLP.")
    return parser.parse_args()


def train_eval(
    cfg: dict,
    hidden_units: int,
    Xtr: np.ndarray,
    ytr: np.ndarray,
    Xte: np.ndarray,
    yte: np.ndarray,
    max_iter: int,
    random_state: int,
) -> tuple[MLPClassifier, float]:
    clf = MLPClassifier(
        hidden_layer_sizes=(hidden_units,),
        activation="relu",
        alpha=1e-4,
        max_iter=max_iter,
        early_stopping=True,
        n_iter_no_change=10,
        random_state=random_state,
        **cfg,
    )
    clf.fit(Xtr, ytr)
    y_pred = clf.predict(Xte)
    acc = accuracy_score(yte, y_pred)
    return clf, acc


def sanitize_identifier(name: str) -> str:
    cleaned = re.sub(r"\W+", "_", name)
    if not cleaned:
        cleaned = "model"
    if cleaned[0].isdigit():
        cleaned = f"_{cleaned}"
    return cleaned


def write_float_array(f, name: str, arr: np.ndarray) -> None:
    data = np.asarray(arr, dtype=np.float32)
    if data.ndim == 1:
        f.write(f"static const float {name}[{data.size}] = {{\n")
        for i in range(0, data.size, 8):
            chunk = ", ".join(f"{v:.8e}f" for v in data[i : i + 8])
            f.write(f"    {chunk},\n")
        f.write("};\n\n")
    elif data.ndim == 2:
        rows, cols = data.shape
        f.write(f"static const float {name}[{rows}][{cols}] = {{\n")
        for r in range(rows):
            row_vals = ", ".join(f"{v:.8e}f" for v in data[r])
            f.write(f"    {{{row_vals}}},\n")
        f.write("};\n\n")
    else:
        raise ValueError("Only 1D or 2D arrays are supported for export.")


def write_int_array(f, name: str, arr: Iterable[int]) -> None:
    values = list(int(v) for v in arr)
    f.write(f"static const int {name}[{len(values)}] = {{\n")
    for i in range(0, len(values), 16):
        chunk = ", ".join(str(v) for v in values[i : i + 16])
        f.write(f"    {chunk},\n")
    f.write("};\n\n")


def write_header(
    path: pathlib.Path,
    prefix: str,
    n_in: int,
    n_hidden: int,
    n_out: int,
    W1: np.ndarray,
    b1: np.ndarray,
    W2: np.ndarray,
    b2: np.ndarray,
    feature_mean: Sequence[float],
    feature_scale: Sequence[float],
    classes: Sequence[int],
) -> None:
    ident = sanitize_identifier(prefix)
    macro_prefix = ident.upper()

    with path.open("w", encoding="utf-8") as f:
        f.write("// Auto-generated by train.py.\n")
        f.write("#pragma once\n\n")
        f.write(f"#define {macro_prefix}_N_IN {n_in}\n")
        f.write(f"#define {macro_prefix}_N_HIDDEN {n_hidden}\n")
        f.write(f"#define {macro_prefix}_N_OUT {n_out}\n")
        f.write(f"#define {macro_prefix}_N_CLASSES {len(classes)}\n\n")

        write_float_array(f, f"{ident}_W1", W1)
        write_float_array(f, f"{ident}_B1", b1)
        write_float_array(f, f"{ident}_W2", W2)
        write_float_array(f, f"{ident}_B2", b2)
        write_float_array(f, f"{ident}_FEATURE_MEAN", np.asarray(feature_mean))
        write_float_array(f, f"{ident}_FEATURE_SCALE", np.asarray(feature_scale))
        write_int_array(f, f"{ident}_CLASSES", classes)


def parse_input_vector(spec: str) -> np.ndarray:
    candidate = pathlib.Path(spec)
    if candidate.exists():
        text = candidate.read_text(encoding="utf-8").strip()
        try:
            values = json.loads(text)
        except json.JSONDecodeError as exc:
            raise ValueError(f"Could not parse JSON from {candidate}: {exc}") from exc
        arr = np.asarray(values, dtype=np.float32)
        if arr.ndim != 1:
            raise ValueError("Input JSON must be a flat array of numbers.")
        return arr

    parts = [p for p in spec.replace(";", ",").split(",") if p.strip()]
    if not parts:
        raise ValueError("Empty --input specification.")
    try:
        return np.array([float(p) for p in parts], dtype=np.float32)
    except ValueError as exc:
        raise ValueError("Could not parse --input values as floats.") from exc


def manual_forward(
    scaled_x: np.ndarray,
    W1: np.ndarray,
    b1: np.ndarray,
    W2: np.ndarray,
    b2: np.ndarray,
) -> tuple[np.ndarray, np.ndarray, np.ndarray]:
    scaled_vec = np.asarray(scaled_x, dtype=np.float32).reshape(-1)
    hidden_pre = scaled_vec @ W1 + b1
    hidden = np.maximum(hidden_pre, 0.0)
    hidden = hidden.astype(np.float32, copy=False)
    logits = (hidden @ W2 + b2).astype(np.float32)
    shifted = logits - np.max(logits)
    exp = np.exp(shifted)
    probs = (exp / np.sum(exp)).astype(np.float32)
    return hidden, logits, probs


def main() -> None:
    args = parse_args()

    informative = min(args.n_informative, args.n_features)
    X, y = make_classification(
        n_samples=args.n_samples,
        n_features=args.n_features,
        n_informative=informative,
        n_redundant=0,
        n_classes=args.n_classes,
        random_state=args.random_state,
    )
    Xtr, Xte, ytr, yte = train_test_split(
        X,
        y,
        test_size=args.test_size,
        stratify=y,
        random_state=args.random_state,
    )

    scaler = StandardScaler().fit(Xtr)
    Xtr_s = scaler.transform(Xtr)
    Xte_s = scaler.transform(Xte)

    if args.solver == "auto":
        # Auto permanece apenas com Adam após a análise de desempenho.
        solver_names = ("adam",)
    else:
        solver_names = (args.solver,)

    candidates = []
    for name in solver_names:
        clf, acc = train_eval(
            SOLVER_CONFIGS[name],
            args.hidden_units,
            Xtr_s,
            ytr,
            Xte_s,
            yte,
            args.max_iter,
            args.random_state,
        )
        print(f"[VAL] solver={name:<4} acc={acc:.4f} iter={clf.n_iter_}")
        candidates.append((name, clf, acc))

    best_name, clf, best_acc = max(candidates, key=lambda item: item[2])
    print(f"[BEST] solver={best_name} acc={best_acc:.4f}")

    n_in = clf.coefs_[0].shape[0]
    n_hidden = clf.coefs_[0].shape[1]
    n_out = clf.coefs_[1].shape[1]

    W1 = clf.coefs_[0].astype(np.float32)
    b1 = clf.intercepts_[0].astype(np.float32)
    W2 = clf.coefs_[1].astype(np.float32)
    b2 = clf.intercepts_[1].astype(np.float32)

    outdir = args.output_dir
    outdir.mkdir(parents=True, exist_ok=True)
    prefix = args.export_prefix

    meta = {
        "solver": best_name,
        "acc_test": float(best_acc),
        "n_in": n_in,
        "n_hidden": n_hidden,
        "n_out": n_out,
        "classes": clf.classes_.astype(int).tolist(),
        "feature_mean": scaler.mean_.astype(np.float32).tolist(),
        "feature_scale": scaler.scale_.astype(np.float32).tolist(),
        "max_iter": args.max_iter,
        "random_state": args.random_state,
    }
    meta_path = outdir / f"{prefix}_meta.json"
    meta_path.write_text(json.dumps(meta, indent=2), encoding="utf-8")

    header_path = outdir / f"{prefix}.h"
    write_header(
        header_path,
        prefix,
        n_in,
        n_hidden,
        n_out,
        W1,
        b1,
        W2,
        b2,
        scaler.mean_.astype(np.float32),
        scaler.scale_.astype(np.float32),
        clf.classes_.astype(int),
    )

    if args.input:
        raw_vec = parse_input_vector(args.input)
        if raw_vec.size != n_in:
            raise ValueError(
                f"Provided vector has {raw_vec.size} elements but model expects {n_in}."
            )
        raw_vec = raw_vec.astype(np.float32)
        scaled = scaler.transform(raw_vec.reshape(1, -1))[0]
        scaled_f32 = scaled.astype(np.float32)
        hidden, logits, probs = manual_forward(scaled_f32, W1, b1, W2, b2)
        skl_probs = clf.predict_proba(scaled.reshape(1, -1))[0]
        if not np.allclose(probs, skl_probs, atol=1e-5):
            raise RuntimeError("Manual forward pass diverged from sklearn outputs.")
        pred_idx = int(np.argmax(probs))
        pred_class = int(clf.classes_[pred_idx])
        print(f"[INFER] class={pred_class} prob={probs[pred_idx]:.4f}")

        sample = {
            "input_raw": raw_vec.astype(float).tolist(),
            "input_scaled": scaled_f32.astype(float).tolist(),
            "hidden": hidden.astype(float).tolist(),
            "logits": logits.astype(float).tolist(),
            "probabilities": probs.astype(float).tolist(),
            "predicted_class": pred_class,
        }
        sample_path = outdir / f"{prefix}_sample.json"
        sample_path.write_text(json.dumps(sample, indent=2), encoding="utf-8")


if __name__ == "__main__":
    main()
